%
%
% Kapitel Datensimulation
%
%


\chapter{Datensimulation}

\section{Monte-Carlo-Simulation}

Ein Vergleich der Ergebnisse aus von CMS gemessenen Daten mit Monte-Carlo-Simulationen ist eine gute Methode, die Messergebnisse mit Vorhersagen des Standardmodells zu vergleichen. Ebenso können so Modelle, die davon abweichen, überprüft werden. Das Ziel der Datensimulation ist es, mithilfe von Zufallsgeneratoren und physikalischer Modelle Pseudodaten zu erzeugen. Diese Pseudodaten liegen dann im selben Datenformat vor, in dem auch die echten Daten zur Verfügung stehen. Somit können dieselben Analysen sowohl auf gemessene als auch auf simulierte Daten angewendet werden. In diesem Kapitel wird erläutert, wie die in dieser Analyse untersuchten simulierten Ereignisse produziert werden.

\subsection{Produktionskette}

Die Simulation der Daten erfolgt in mehreren, aufeinander aufbauenden Schritten. Diese sind im Einzelnen:

\begin{enumerate}
	\item Numerische Integration des Matrixelementes (ME) des harten Prozesses und Berechnung des Wirkungsquerschnittes.
	\item Ereignisgeneration: Partonen und Leptonen im Anfangs- und Endzustand der harten Interaktion werden, unter Beachtung der erwarteten Wahrscheinlichkeiten, zufällig erzeugt.
	\item Modellierung von Teilchenschauern: Die Abstrahlung von Gluonen und Photonen aus den Teilchen im Anfangs- und Endzustand wird simuliert.
	\item Hadronisierung: Gruppierung von Quarks zu farbneutralen Hadronen.
	\item Hadronischer Zerfall: Der Zerfall von kurzlebigen Teilchen wird reproduziert.
	\item Underlying Event (UE): Hinzufügen von Teilchen im Endzustand, die von Protonrückständen aus dem harten Prozess entstehen.
	\item Pileup: Eine zufällige Anzahl von zusätzlichen, weichen Proton-Proton-Kollisionen wird erzeugt und dem Ereignis hinzugefügt.
	\item Detektorsimulation: Die Wechselwirkung der in Schritt 2 bis 7 produzierten Teilchen mit dem Detektormaterial und die Detektorantwort werden simuliert.
\end{enumerate}

\subsection{Simulation von \texorpdfstring{$t\overline{t}+\gamma$}{ttg} Ereignissen}

Für die Simulation des $t\overline{t}+\gamma$-Signals wurde WHIZARD, ein Leading-Order-Monte-Carlo-Generator, benutzt. Ein Schaubild mit verschiedenen simulierbaren Endzuständen der Monte-Carlo-Generation ist in Abb.\ref{fig:whizard} gezeigt, auf diese soll hier kurz eingegangen werden:

\begin{description}
  \item[2 $\rightarrow$ 3:] Hier werden nur quantenmechanische Interferenzen aus Photonabstrahlungen von Teilchen im Anfangszustand betrachtet. Die Vorteile dieses Modells sind die geringe CPU-Last, die bei der Berechnung der ME anfällt und die genaue Zuordnung der Photonen zum Top-Vertex. Es zeigt sich aber, dass die Physik hier nicht korrekt beschrieben wird. \cite{Tholen:Master}
	\item[2 $\rightarrow$ 5:] In diesem Modell ist der Zerfall der Top-Quarks berücksichtigt, hier trägt nun auch die Photonabstrahlung der W-Bosonen und der b-Quarks zum Signal bei. Es stellt einen guten Kompromiss dar zwischen CPU-Auslastung und korrekter Beschreibung der Natur und wird in dieser Analyse benutzt.
	\item[2 $\rightarrow$ 7:] Hier werden alle Photonabstrahlungen des harten Prozesses berücksichtigt und die realen Prozesse am präzisesten beschrieben, diese Strategie ist jedoch überaus CPU-intensiv und wird in dieser Analyse nicht weiter verwendet.
\end{description}

\begin{figure}%
\centering
\includegraphics[width=0.7\columnwidth]{bilder/whizard}%
\caption{Strategien der Ereignisgeneration mit WHIZARD. \cite{Tholen:Master}}%
\label{fig:whizard}%
\end{figure}

WHIZARD benutzt die Partondichteverteilung CTEQ6L1 \cite{Pumplin:CTEQ} und eine variable Renorma\-lisierungs- und Faktorisierungsskala. Diese Skalen werden für jedes Ereignis auf einen Wert von 172,5\,GeV ($m_t$) plus der Transversalenergie des erzeugten Photons festgelegt. Die Teilchenschauer der Anfangs- und Endzustände sowie die Hadronisierung werden von PYTHIA6 \cite{Sjostrand:PYTHIA}, TAUOLA und PHOTOS (beide in \cite{Was:TAUOLA}) simuliert, dabei wird dieselbe Konfiguration wie für das Top-Paar-Sample (siehe auch Abschnitt \ref{sec:sim_top_paar} benutzt. \\
An die Teilchen im Endzustand werden bestimmte Anforderungen gestellt, sogenannte \enquote{Produktionsschnitte}. So wird eine minimale transversale Energie gefordert, um Infrarotdivergenz zu vermeiden und eine Minimaldistanz im $\eta-\Phi$-Raum gegen kollineare Divergenz. In dieser Analyse wird eine minimale Transversalenergie des Photons und beider b-Quarks von 10\,GeV und ein $\Delta R > 0,1$ zwischen dem Photon und jedem anderen Teilchen im Endzustand gefordert. Die Messung wird davon nicht beeinflusst, da die Schnitte in der Ereignisselektion noch härter sind, siehe Kapitel \ref{sec:selektion}.

\subsection{Simulation des Top-Paar-Prozesses und der betrachteten Untergründe}
\label{sec:sim_top_paar}

Die meisten Monte-Carlo-Simulationen neben den $t\overline{t}+\gamma$-Ereignissen werden mit MADGRAPH \cite{Alwall:MADGRAPH} generiert, hier wird die Partondichtefunktion CTEQ6L1 benutzt. Single-Top-Ereignisse werden mit POWHEG \cite{Alioli:POWHEG}, \cite{Re:POWHEG} simuliert, der Zerfall des $\tau$-Leptons mittels TAUOLA. Die Hadronisierung sowie die Teilchenschauer werden mit PYTHIA (hadronische Wechselwirkung) sowie PHOTOS (elektromagnetische Wechselwirkung) modelliert.\
Der Phasenraum der Ereignisse der WHIZARD $t\overline{t}+\gamma$-Simulation ist im MADGRAPH $t\overline{t}$-Sample schon abgedeckt. Deswegen werden Ereignisse aus dem $t\overline{t}$-Sample, welche die Produktionsschnitte in WHIZARD auf Partonebene erfüllen, nicht berücksichtigt, um diese Überschneidung zu entfernen.

\subsection{Detektorsimulation}

Der Durchgang der von den Monte-Carlo-Generatoren erzeugten Teilchen durch die einzelnen Detektorkomponenten sowie die dadurch ausgelösten Signale werden mit dem Programm GEANT4 \cite{Agostinelli:GEANT} modelliert. GEANT4 simuliert anhand der Detektorgeometrie und des Magnetfeldes das Verhalten der einzelnen Teilchen im Detektor. Dabei werden alle elektromagnetischen und hadronischen Wechselwirkungen mit dem Detektormaterial berücksichtigt, wie z.B. Schauer in den Kalorimetern. Trifft ein Teilchen aktives Detektormaterial, erzeugt die Simulation das elektrische Signal, welches auch bei einer Messung entstehen würde und simuliert die elektronische Auslese der Signale, die Digitalisierung, in den unterschiedlichen Detektorkomponenten. Die Ausgabe der Digitalisierung wird gespeichert und später in der Rekonstruktion der einzelnen Teilchen verwendet. Diese Rekonstruktion erfolgt mit den gleichen Algorithmen wie die Rekonstruktion experimenteller Daten. Neben der vollständigen Simulation des Detektors (\enquote{FullSim}), die viel Rechenzeit und -aufwand benötigt, wurden auch vereinfachte Modelle zur Detektorsimulation entwickelt (\enquote{FastSim}) \cite{CMS:TDR1}. Dadurch werden die Dauer der Simulation und der benötigte Speicherplatz erheblich reduziert. Die Daten in dieser Analyse werden mittels FullSim prozessiert.


\section{Studie der von WHIZARD generierten Monte-Carlo-Samples}
\label{sec:montecarlo}

Die Studie der von WHIZARD generierten Monte-Carlo-Daten soll zeigen, wie sich der Wirkungsquerschnitt des $t\overline{t}+\gamma$-Prozesses sowie das $E_T$-Spektrum der Photonen aus dem harten Prozess durch Variation des angenommenen $d_A^{\gamma}$-Wertes verändert. Des weiteren soll untersucht werden, welche Variable sich am besten dazu eignet, zwischen verschiedenen $d_A^{\gamma}$-Szenarien zu unterscheiden. Durch Verwendung von Generatorteilchen werden Auflösungseffekte durch die Rekonstruktion sowie Akzeptanzeffekte durch die Selektion vernachlässigt.

\subsection{Wirkungsquerschnitt des \texorpdfstring{$t\overline{t}+\gamma$}{ttg}-Prozesses}
\label{sec:wq}

Es wird eine quadratische Abhängigkeit des Wirkungsquerschnittes vom Parameter $d_A^{\gamma}$ erwartet, da $d_A^{\gamma}$ linear in das Matrixelement eingeht und der Wirkungsquerschnitt proportional zum Quadrat des Matrixelementes ist. \\
Mit dem Leading-Order-Monte-Carlo-Generator WHIZARD in der Version 2.1.1 werden für Werte für das elektrische Dipolmoment von $0<d_A^{\gamma}<1$ in Schritten von 0,01 zunächst die Matrixelemente für den harten Prozess generiert. Der Ansatz $2 \rightarrow 5$ wird implementiert mit zwei W-Bosonen, zwei b-Quarks und einem Photon im Endzustand. Die Matrixelement-Berechnung wird so lange fortgeführt, bis ein Fehler auf den Wirkungsquerschnitt von unter 1\% errechnet wird. Die so berechneten Wirkungsquerschnitte sind in Abb. \ref{fig:cs_wofit} dargestellt. Eine Anpassung zeigt die erwartete quadratische Abhängigkeit. Auffällig ist der große Fehler auf den Wirkungsquerschnitt bei den Werten $d_A^{\gamma}=0,06$ und $d_A^{\gamma}=0,95$. Dieser lässt sich auch bei deutlich längerer Berechnung des Matrixelements nicht weiter verringern. Es ist derzeit nicht klar, was diesen großen Fehler verursacht. Es wird von einem internen Berechnungsproblem von WHIZARD bei diesen $d_A^{\gamma}$-Werten ausgegangen, da sich die Steuerungsskripte zur Berechnung der verschiedenen Monte-Carlo-Samples ausschließlich im vorgegebenen $d_A^{\gamma}$-Wert unterscheiden, ein Fehler in diesen Skripten also auszuschließen ist.

\begin{figure}%
\centering
\includegraphics[width=0.5\columnwidth]{bilder/crosssectionplot}%
\caption{Wirkungsquerschnitt des $t\overline{t}+\gamma$-Prozesses für $0<d_A^{\gamma}<1$ mit eingezeichneter Fit-Funktion}%
\label{fig:cs_wofit}%
\end{figure}

\subsection{\texorpdfstring{$E_T$}{ET}-Spektrum des Photons}

Aus dem berechneten Matrixelement werden für jeden $d_A^{\gamma}$-Wert 105.000 Ereignisse generiert und im Les-Houches-Event Format (LHEF) abgespeichert \cite{Alwall:LesHouches}. Abbildung \ref{fig:et_gen_galerie} zeigt in halblogarithmischer Auftragung ausgewählte $E_T$-Spektren dieser generierten Ereignisse. Es bestätigt sich hier die Erwartung, dass sich das Photonspektrum bei einer höheren Kopplungsstärke $d_A^{\gamma}$ zu höheren Energien hin verschiebt. Im folgenden werden mehrere Ansätze diskutiert, die Abhängigkeit des $E_T$-Spektrums von $d_A^{\gamma}$ zu beschreiben.

\begin{figure}%
\begin{subfigure}[b]{0.4\textwidth}
\includegraphics[width=\textwidth]{bilder/et_gen_0}%
\end{subfigure}
\hspace{0.1\textwidth}
\begin{subfigure}[b]{0.4\textwidth}
\includegraphics[width=\textwidth]{bilder/et_gen_02}%
\end{subfigure}

\begin{subfigure}[b]{0.4\textwidth}
\includegraphics[width=\textwidth]{bilder/et_gen_04}%
\end{subfigure}
\hspace{0.1\textwidth}
\begin{subfigure}[b]{0.4\textwidth}
\includegraphics[width=\textwidth]{bilder/et_gen_06}%
\end{subfigure}

\begin{subfigure}[b]{0.4\textwidth}
\includegraphics[width=\textwidth]{bilder/et_gen_08}%
\end{subfigure}
\hspace{0.1\textwidth}
\begin{subfigure}[b]{0.4\textwidth}
\includegraphics[width=\textwidth]{bilder/et_gen_1}%
\end{subfigure}
\caption{$E_T$-Verteilungen von Monte-Carlo-Daten auf Generator-Level für verschiedene Werte von $d_A^{\gamma}$.}%
\label{fig:et_gen_galerie}%
\end{figure}

\subsubsection{2-Bin-Analyse}

Das $E_T$-Spektrum wird in zwei Bereiche aufgeteilt. Der erste Bereich reicht von 0\,GeV bis 100\,GeV, der zweite von 100\,GeV bis 700\,GeV, siehe dazu Abb. \ref{fig:2bin_skizze}. Nun wird das Verhältnis der Anzahl der Photonen in den beiden Bereichen gegen $d_A^{\gamma}$ aufgetragen, siehe Abb. \ref{fig:2bin_plot}. Auf der linken Seite sieht man zwei Messwerte, die nicht dem allgemeinen Verlauf folgen. Dies sind die schon in Kapitel \ref{sec:wq} angesprochenen Messpunkte, die durch irreduzibel große Fehler auf den Wirkungsquerschnitt auffielen und auch in allen anderen Analysemethoden herausstechen. Die Punkte $d_A^{\gamma}=0,06$ und $d_A^{\gamma}=0,95$ werden im folgenden für die Auswertung der verschiedenen Analysemethoden nicht berücksichtigt. Der so bereinigte Plot ist in Abb. \ref{fig:2bin_plot} rechts zu sehen. Die 2-Bin-Analyse stellt einen vielversprechenden Ansatz dar, das härtere Photonenspektrum für größere Kopplungsstärken $d_A^{\gamma}$ zu beschreiben. Auf Generatorniveau ist eine sehr gute Separationskraft vorhanden, der Wertebereich erstreckt sich von $E_{Low} / E_{High} = 3,8$ für $d_A^{\gamma} = 0$ bis $E_{Low} / E_{High} = 2,2$ für $d_A^{\gamma} = 1$ bei einer Unsicherheit von $\Delta E_{Low} / E_{High} \leq 0,03$. 

\begin{figure}%
\centering
\includegraphics[width=0.5\columnwidth]{bilder/2binskizze}%
\caption{Schema der Unterteilung bei der 2-Bin-Analyse.}%
\label{fig:2bin_skizze}%
\end{figure}

\begin{figure}%
\centering
\begin{subfigure}[b]{0.4\textwidth}
  \includegraphics[width=\textwidth]{bilder/twobinplot}%
\end{subfigure}
\hspace{0.1\textwidth}
\begin{subfigure}[b]{0.4\textwidth}
  \includegraphics[width=\textwidth]{bilder/twobinplotv2}%
\end{subfigure}
\caption{Verhältnis der Anzahl der Photonen in Bin 1 und Bin 2. Im rechten Plot sind die Werte für $d_A^{\gamma} = 0,06$ und $d_A^{\gamma} = 0,95$ nicht berücksichtigt.}%
\label{fig:2bin_plot}%
\end{figure}

\subsubsection{\texorpdfstring{$E_T$}{ET}-Mean-Analyse}

In diesem Ansatz wird der Schwerpunkt der $E_T$-Verteilung betrachtet, dies ist der gewichtete Mittelwert $\overline{E_T}$ dieser Verteilung. Für die Berechnung des gewichteten Mittelwertes und des Fehlers auf diesen Mittelwert werden die folgenden Formeln benutzt:

\begin{align}
\overline{E_T} &= \frac{1}{N} \sum{x_i\cdot \omega_i} \\
\Delta \overline{E_T} &= \frac{\sum{(x_i - \overline{E_T})^2 \cdot \omega_i}}{\sqrt{N}} \ .
\end{align}

Hier ist $x_i$ der Mittelpunkt des i-ten Bins des Histogrammes und $\omega_i$ die Anzahl der Einträge in diesem Bin. $E_{T,mean}$ wird gegen $d_A^{\gamma}$ aufgetragen (siehe Abbildung \ref{fig:mean_plot}). Man sieht wiederum eine hohe Separationskraft dieser Variablen auf Generatorniveau, der Wertebereich liegt hier zwischen 34,7 und 53,5\,GeV mit einer Unsicherheit von $\Delta \overline{E_T} \leq 0,22$. 

\begin{figure}%
\centering
\includegraphics[width=0.5\columnwidth]{bilder/meanplot}%
\caption{Verlauf des Schwerpunktes der $E_T$-Verteilung.}%
\label{fig:mean_plot}%
\end{figure}

\subsubsection{Analyse der Exponentialanpassung der \texorpdfstring{$E_T$}{ET}-Verteilung}
\label{sec:ana_slope}

\begin{figure}%
\centering
\includegraphics[width=0.5\columnwidth]{bilder/slopeskizze}%
\caption{Skizze zum Prinzip der Exponentialanpassung.}%
\label{fig:slope_skizze}%
\end{figure}

Hier wird die Anpassung einer Exponentialkurve an die $E_T$-Verteilung betrachtet. Wie in Abb. \ref{fig:slope_skizze} zu sehen ist, kann die Verteilung im Bereich zwischen 100\,GeV und 225\,GeV durch eine logarithmische Funktion

\begin{equation}
N(x) = N_0\cdot e^{\lambda \cdot x}
\end{equation}

beschrieben werden. Für jeden $d_A^{\gamma}$-Wert wird nun an das $E_T$-Spektrum eine Exponentialfunktion angepasst und der Wert von $\lambda$ gegen $d_A^{\gamma}$ aufgetragen. Dieser Plot ist in Abb. \ref{fig:slope_plot} zu sehen. Auch hier ist ein eindeutiger Trend zu erkennen. Die $\lambda$-Werte reichen von -0,0115 bis -0,0218 bei einer Unsicherheit von $\Delta \lambda \leq 0,00053$.

\begin{figure}%
\centering
\includegraphics[width=0.5\columnwidth]{bilder/slopeplot}%
\caption{Verlauf des Exponentialkoeffizienten $\lambda$ der Anpassung an die $E_T$-Verteilung.}%
\label{fig:slope_plot}%
\end{figure}

\subsubsection{Vergleich der verschiedenen Analysemethoden}

Alle drei vorgestellten Analysemethoden zeigen auf Generatorniveau eine klare Abhängigkeit der untersuchten Variablen von $d_A^{\gamma}$. Dabei weisen die 2-Bin-Analyse sowie die Analyse des Schwerpunktes des Photon-$E_T$-Spektrums die größte Separationskraft auf. In der 2-Bin-Analyse liegen zwischen dem Minimal- und dem Maximalwert von $E_{Low}/E_{High}$ rund 50 Standardabweichungen, bei der Analyse von $\overline{E_T}$ sogar 85. Die Analyse der Exponentialanpassung des Photonspektrums weist eine etwas schwächere Separationskraft auf, hier liegen zwischen den Extremwerten von $\lambda$ 19 Standardabweichungen. \\
Die Unsicherheiten auf die Variablen bei der 2-Bin-Analyse und der Mean-Analyse sind proportional zu $\sqrt{N}$. Es wird demnach für die Analyse der selektierten Monte-Carlo-Ereignisse und insbesondere für die Analyse von gemessenen Daten eine größere Unsicherheit auf die jeweiligen Variablen erwartet. Bei einer Verringerung von 105.000 Ereignissen (die hier vorgestellte Analyse auf Generatorniveau) auf beispielsweise nur noch 1050 Ereignisse ist die 10-fache Unsicherheit auf die Werte von $E_{Low}/E_{High}$ und $\lambda$ zu erwarten. Wie sehr die Unsicherheit auf die Exponentialanpassung von der Anzahl der untersuchten Ereignisse abhängt, ist weitaus schwieriger zu quantifizieren, es wird aber davon ausgegangen, dass die Abhängigkeit nicht so stark ist wie bei den anderen beiden Analysen. Des weiteren werden zu den hier besprochenen statistischen Unsicherheiten bei der Rekonstruktion und Selektion auch systematische Unsicherheiten entstehen, welche die Separationskraft der analysierten Variablen weiter abnehmen lassen wird.